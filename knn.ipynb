{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qdr6G5_tHfZ"
      },
      "source": [
        "questions:\n",
        "1. what is logistic regression used for in this project?\n",
        "2. what is naive bayes used for in this project?\n",
        "3. what metrics and attributes are we using with knn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kphjSEUIw6Y"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dHY5x8MIIyg_"
      },
      "outputs": [],
      "source": [
        "#import pandas for table and csv handling, randomizing dataset\n",
        "import pandas\n",
        "\n",
        "#import numpy for fast math calculations\n",
        "import numpy\n",
        "\n",
        "#import KFold from SKLearn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#import accuracu from SKLearn\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQz3hQKuBYIH"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ha0ozsPQJP4d"
      },
      "outputs": [],
      "source": [
        "def importData(filename):\n",
        "\n",
        "  #import file\n",
        "  return pandas.read_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def removeAttributes(dataset):\n",
        "\n",
        "    #drop capital run length average, longest, and total\n",
        "    dataset = dataset.drop(\"capital_run_length_average\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_longest\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_total\", axis='columns')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roundToOne(dataset):\n",
        "    return dataset.mask(dataset > 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamOccurrences(dataset):\n",
        "\n",
        "    #positive = spam\n",
        "    #negative = not spam\n",
        "\n",
        "    #get all rows where spam value is set to 1 and 0 respectively\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    #get total occurrences of spam, not spam, and total\n",
        "    spam_length = len(spam.index)\n",
        "    not_spam_length = len(not_spam.index)\n",
        "    total_length = spam_length + not_spam_length\n",
        "\n",
        "    #return tuple with the number of spam and not spam emails in the dataset\n",
        "    return (spam_length, not_spam_length, total_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamProbability(dataset):\n",
        "    spam, not_spam, total = getSpamOccurrences(dataset)\n",
        "    return (spam / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDatasetLength(dataset):\n",
        "    return len(dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordProbabilities(dataset):\n",
        "\n",
        "    #round all non-zero values to 1\n",
        "    dataset = roundToOne(dataset)\n",
        "\n",
        "    #define dictionary for storing word probabilities\n",
        "    word_probability = dict()\n",
        "\n",
        "    #get number of unique features (for laplace smoothing)\n",
        "    unique = len(dataset.columns)\n",
        "\n",
        "    #iterate through columns\n",
        "    for column in dataset.columns:\n",
        "\n",
        "        #sum all values in the current column\n",
        "        #add 1 for laplace smoothing\n",
        "        #divide by the dataset length\n",
        "        #add number of unique values (number of columns)\n",
        "        #to the dataset length for laplace smoothing\n",
        "        word_probability[column] = ((dataset[column].sum() + 1) \n",
        "        / (getDatasetLength(dataset) + unique))\n",
        "\n",
        "    #return word probability dictionary\n",
        "    return word_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamWordProbabilities(dataset):\n",
        "\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    spam_probabilities = getWordProbabilities(spam)\n",
        "    not_spam_probabilities = getWordProbabilities(not_spam)\n",
        "\n",
        "    return (spam_probabilities, not_spam_probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpCcCVBBexF"
      },
      "source": [
        "# KNN\n",
        "Use parallel programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelEvaluation(dataset, spam_probabilities, not_spam_probabilities, spam_probability):\n",
        "\n",
        "    #get spam probability\n",
        "\n",
        "    predicted_list = []\n",
        "\n",
        "    for index, email in dataset.iterrows():\n",
        "\n",
        "        #reset spam predicted\n",
        "        spam_predicted = 1\n",
        "        not_spam_predicted = 1\n",
        "\n",
        "        for column in dataset.iloc[:,:-1]:\n",
        "\n",
        "            if email[column] > 0.:\n",
        "                spam_predicted *= spam_probabilities[column]\n",
        "                not_spam_predicted *= not_spam_probabilities[column]\n",
        "                \n",
        "        spam_predicted = spam_predicted * spam_probability\n",
        "        not_spam_predicted = not_spam_predicted * (1 - spam_probability)\n",
        "\n",
        "        if spam_predicted > not_spam_predicted:\n",
        "            predicted_list.append(1)\n",
        "        else:\n",
        "            predicted_list.append(0)\n",
        "\n",
        "    print(accuracy_score(predicted_list, dataset['spam']))\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNN:\n",
        "\n",
        "    def __init__(self, dataset, k=3):\n",
        "        self.k = k\n",
        "\n",
        "        #x_train = attributes\n",
        "        self.x_train = dataset.iloc[:,:-1]\n",
        "\n",
        "        #y_train = label\n",
        "        self.y_train = dataset.iloc[:,-1]\n",
        "\n",
        "    def _cosine_distance(self, vector1, vector2):\n",
        "\n",
        "        #define subfunction for getting dot product\n",
        "        #a dot product is the sum of the product of all components of two vectors\n",
        "        #print(vector1, vector2)\n",
        "        return (1 - distance.cosine(vector1, vector2))\n",
        "\n",
        "    def predict(self, testing):\n",
        "        testing = testing.iloc[:,:-1]\n",
        "        print(\"starting...\")\n",
        "        distances = cosine_distances(self.x_train, testing)\n",
        "        #get k closest\n",
        "        print(\"ending...\")\n",
        "        print(distances)\n",
        "\n",
        "\n",
        "    def getDistances(self, x):\n",
        "        #compute distance using cosine\n",
        "        distances = [self._cosine_distance(x, x_train) for index, x_train in self.x_train.iterrows()]\n",
        "\n",
        "        #get closest k\n",
        "        k_indices = numpy.argsort(distances)[:self.k]\n",
        "        k_labels = [self.y_train.to_numpy()[index] for index in k_indices]\n",
        "        \n",
        "        return max(set(k_labels), key=k_labels.count)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as multi\n",
        "\n",
        "import dill\n",
        "\n",
        "class KNN2:\n",
        "\n",
        "    def __init__(self, dataset, k=5):\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        self.training = dataset.iloc[:,:-1]\n",
        "        self.label = dataset.iloc[:,-1]\n",
        "\n",
        "    def distance(self, vector1, vector2):\n",
        "\n",
        "        dot = numpy.dot\n",
        "        norm = numpy.linalg.norm\n",
        "\n",
        "        cos_similarity = (dot(vector1, vector2) / (norm(vector1) * norm(vector2)))\n",
        "        return (1 - cos_similarity)\n",
        "    \n",
        "    def getDistancesFromPoint(self, vector):\n",
        "\n",
        "        #the vector is essentially a single row in the dataframe\n",
        "        #all attributes except those that have been dropped\n",
        "        #and the label itself are used as part of the cosine distance formula\n",
        "        distances = []\n",
        "\n",
        "        #iterate through rows in dataframe\n",
        "        #for index in self.dataset[0].index:\n",
        "\n",
        "        for index in self.training.index:\n",
        "\n",
        "            #get current point from dataset to get distance from\n",
        "            vector2 = self.training.iloc[index]\n",
        "\n",
        "            #calculate distance\n",
        "            distance = self.distance(vector, vector2)\n",
        "\n",
        "            #append to distance list in dataset\n",
        "            distances.append((distance, self.label[index]))\n",
        "\n",
        "        #return distances\n",
        "        return distances\n",
        "    \n",
        "    def testfunc(self, chunk, vector):\n",
        "        return 1\n",
        "\n",
        "    def multiDistances(self, vector, num_processes=10):\n",
        "\n",
        "\n",
        "\n",
        "        def splitIntoChunks(dataset, num_processes):\n",
        "\n",
        "            dataset_size = len(dataset.index)\n",
        "            chunk_size = dataset_size // num_processes\n",
        "            chunks = []\n",
        "\n",
        "            #iterate from start to end of dataset by chunk_size\n",
        "            for index in range(0, dataset_size, chunk_size):\n",
        "                chunks.append(dataset.iloc[index:index + chunk_size])\n",
        "\n",
        "            return chunks\n",
        "\n",
        "        \n",
        "        chunks = splitIntoChunks(self.training, num_processes)\n",
        "\n",
        "        pool = multi.Pool(processes=num_processes)\n",
        "\n",
        "        results = pool.starmap(self.testfunc, [(chunk, vector) for chunk in chunks])\n",
        "\n",
        "        #distances = [item for sublist in results for item in sublist]\n",
        "\n",
        "        #pool.close()\n",
        "        #pool.join()\n",
        "\n",
        "        #sort distances list based on \n",
        "\n",
        "        \"\"\"\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        sorted_labels = [label for distance, label in distances]\n",
        "        \"\"\"\n",
        "\n",
        "    def predict(self, vector):\n",
        "\n",
        "        def findMajority(labels):\n",
        "\n",
        "            #create occurrences array\n",
        "            occurrences = []\n",
        "\n",
        "            #create set of all possible labels\n",
        "            label_types = set(labels)\n",
        "\n",
        "            #iterate through label types\n",
        "            for label in label_types:\n",
        "\n",
        "                #append label and count of label\n",
        "                occurrences.append((label, labels.count(label)))\n",
        "\n",
        "            #find majority label by count using lambda function\n",
        "            majority = max(occurrences, key=lambda count: count[1])\n",
        "\n",
        "            #return majority\n",
        "            return majority\n",
        "\n",
        "        #labels = self.getDistancesFromPoint(vector)[:self.k]\n",
        "        self.multiDistances(vector)\n",
        "        #return findMajority(labels)[0]\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process SpawnPoolWorker-366:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-367:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-368:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-369:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-370:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-371:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-372:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-373:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-374:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-375:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n",
            "Process SpawnPoolWorker-376:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "AttributeError: Can't get attribute 'KNN2' on <module '__main__' (built-in)>\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_44168/1370131174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtestKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_44168/1370131174.py\u001b[0m in \u001b[0;36mtestKNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmajority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajority\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_44168/1995071787.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m#labels = self.getDistancesFromPoint(vector)[:self.k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiDistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;31m#return findMajority(labels)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_44168/1995071787.py\u001b[0m in \u001b[0;36mmultiDistances\u001b[0;34m(self, vector, num_processes)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#distances = [item for sublist in results for item in sublist]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         '''\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data = importData(\"spambase.csv\")\n",
        "knn = KNN2(data)\n",
        "\n",
        "def testKNN():\n",
        "\n",
        "    vector1 = data.iloc[:,:-1].iloc[3]\n",
        "    vector2 = data.iloc[:,:-1].iloc[1]\n",
        "\n",
        "    majority = knn.predict(vector2)\n",
        "\n",
        "    print(majority)\n",
        "\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    testKNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(value):\n",
        "    return 1 / (1 + numpy.exp(-value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addOneColumn(dataset):\n",
        "\n",
        "    #return dataset with new column with all values initialized to one\n",
        "    dataset[\"added_column\"] = 1\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getVectorM(lengthOfDataset):\n",
        "\n",
        "    #vector is represented as an array of numbers\n",
        "    #amount of numbers is determined by number of columns in dataset dataframe\n",
        "    #the vector should be one-dimensional\n",
        "\n",
        "    #first parameter = number of elements in first array\n",
        "    #example code has 1 passed in to signify this is a one-dimensional array\n",
        "\n",
        "    return numpy.random.randn(lengthOfDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitData(dataset):\n",
        "\n",
        "    #drop label column for predicted\n",
        "    dataset = dataset.drop(columns=['spam'])\n",
        "    label = dataset['spam']\n",
        "\n",
        "    return dataset, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance(true, predicted):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linearRegression(dataset):\n",
        "\n",
        "    #add column to dataset\n",
        "    dataset = addOneColumn(dataset)\n",
        "\n",
        "    #create randomized vector m\n",
        "    vector_m = getVectorM(len(dataset.columns))\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        #multiply vector_m by dataset\n",
        "        dataset.dot(vector_m)\n",
        "\n",
        "        #apply sigmoid function\n",
        "        predicted_y = dataset.apply(sigmoid)\n",
        "\n",
        "        #each line will have a predicted y\n",
        "        #it will be 1 for each row\n",
        "        (dataset * (predicted_y - dataset)) * (2 / getDatasetLength(dataset))\n",
        "\n",
        "        print(predicted_y)\n",
        "\n",
        "        currentPerformance = performance()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test():\n",
        "    dataset = importData(\"spambase.csv\")\n",
        "    dataset = removeAttributes(dataset)\n",
        "    dataset = addOneColumn(dataset)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVbjUThpBgf8"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "febo9OPVBhai"
      },
      "outputs": [],
      "source": [
        "def testDataset(training, test):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoSSUQuVovxL"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  #split the dataset\n",
        "\n",
        "  #use optimize k to find a k value\n",
        "\n",
        "  #build knn graph\n",
        "\n",
        "  #use the validation dataset to test the accuracy of the program\n",
        "\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
