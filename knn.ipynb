{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qdr6G5_tHfZ"
      },
      "source": [
        "questions:\n",
        "1. what is logistic regression used for in this project?\n",
        "2. what is naive bayes used for in this project?\n",
        "3. what metrics and attributes are we using with knn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kphjSEUIw6Y"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dHY5x8MIIyg_"
      },
      "outputs": [],
      "source": [
        "#import pandas for table and csv handling, randomizing dataset\n",
        "import pandas\n",
        "\n",
        "#import numpy for fast math calculations\n",
        "import numpy\n",
        "\n",
        "#import KFold from SKLearn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#import accuracu from SKLearn\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQz3hQKuBYIH"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ha0ozsPQJP4d"
      },
      "outputs": [],
      "source": [
        "def importData(filename):\n",
        "\n",
        "  #import file\n",
        "  return pandas.read_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def removeAttributes(dataset):\n",
        "\n",
        "    #drop capital run length average, longest, and total\n",
        "    dataset = dataset.drop(\"capital_run_length_average\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_longest\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_total\", axis='columns')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roundToOne(dataset):\n",
        "    return dataset.mask(dataset > 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamOccurrences(dataset):\n",
        "\n",
        "    #positive = spam\n",
        "    #negative = not spam\n",
        "\n",
        "    #get all rows where spam value is set to 1 and 0 respectively\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    #get total occurrences of spam, not spam, and total\n",
        "    spam_length = len(spam.index)\n",
        "    not_spam_length = len(not_spam.index)\n",
        "    total_length = spam_length + not_spam_length\n",
        "\n",
        "    #return tuple with the number of spam and not spam emails in the dataset\n",
        "    return (spam_length, not_spam_length, total_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamProbability(dataset):\n",
        "    spam, not_spam, total = getSpamOccurrences(dataset)\n",
        "    return (spam / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDatasetLength(dataset):\n",
        "    return len(dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordProbabilities(dataset):\n",
        "\n",
        "    #round all non-zero values to 1\n",
        "    dataset = roundToOne(dataset)\n",
        "\n",
        "    #define dictionary for storing word probabilities\n",
        "    word_probability = dict()\n",
        "\n",
        "    #get number of unique features (for laplace smoothing)\n",
        "    unique = len(dataset.columns)\n",
        "\n",
        "    #iterate through columns\n",
        "    for column in dataset.columns:\n",
        "\n",
        "        #sum all values in the current column\n",
        "        #add 1 for laplace smoothing\n",
        "        #divide by the dataset length\n",
        "        #add number of unique values (number of columns)\n",
        "        #to the dataset length for laplace smoothing\n",
        "        word_probability[column] = ((dataset[column].sum() + 1) \n",
        "        / (getDatasetLength(dataset) + unique))\n",
        "\n",
        "    #return word probability dictionary\n",
        "    return word_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamWordProbabilities(dataset):\n",
        "\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    spam_probabilities = getWordProbabilities(spam)\n",
        "    not_spam_probabilities = getWordProbabilities(not_spam)\n",
        "\n",
        "    return (spam_probabilities, not_spam_probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpCcCVBBexF"
      },
      "source": [
        "# KNN\n",
        "Use parallel programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelEvaluation(dataset, spam_probabilities, not_spam_probabilities, spam_probability):\n",
        "\n",
        "    #get spam probability\n",
        "\n",
        "    predicted_list = []\n",
        "\n",
        "    for index, email in dataset.iterrows():\n",
        "\n",
        "        #reset spam predicted\n",
        "        spam_predicted = 1\n",
        "        not_spam_predicted = 1\n",
        "\n",
        "        for column in dataset.iloc[:,:-1]:\n",
        "\n",
        "            if email[column] > 0.:\n",
        "                spam_predicted *= spam_probabilities[column]\n",
        "                not_spam_predicted *= not_spam_probabilities[column]\n",
        "                \n",
        "        spam_predicted = spam_predicted * spam_probability\n",
        "        not_spam_predicted = not_spam_predicted * (1 - spam_probability)\n",
        "\n",
        "        if spam_predicted > not_spam_predicted:\n",
        "            predicted_list.append(1)\n",
        "        else:\n",
        "            predicted_list.append(0)\n",
        "\n",
        "    print(accuracy_score(predicted_list, dataset['spam']))\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNN:\n",
        "\n",
        "    def __init__(self, dataset, k=3):\n",
        "        self.k = k\n",
        "\n",
        "        #x_train = attributes\n",
        "        self.x_train = dataset.iloc[:,:-1]\n",
        "\n",
        "        #y_train = label\n",
        "        self.y_train = dataset.iloc[:,-1]\n",
        "\n",
        "    def _cosine_distance(self, vector1, vector2):\n",
        "\n",
        "        #define subfunction for getting dot product\n",
        "        #a dot product is the sum of the product of all components of two vectors\n",
        "        #print(vector1, vector2)\n",
        "        return (1 - distance.cosine(vector1, vector2))\n",
        "\n",
        "    def predict(self, testing):\n",
        "        testing = testing.iloc[:,:-1]\n",
        "        print(\"starting...\")\n",
        "        distances = cosine_distances(self.x_train, testing)\n",
        "        #get k closest\n",
        "        print(\"ending...\")\n",
        "        print(distances)\n",
        "\n",
        "\n",
        "    def getDistances(self, x):\n",
        "        #compute distance using cosine\n",
        "        distances = [self._cosine_distance(x, x_train) for index, x_train in self.x_train.iterrows()]\n",
        "\n",
        "        #get closest k\n",
        "        k_indices = numpy.argsort(distances)[:self.k]\n",
        "        k_labels = [self.y_train.to_numpy()[index] for index in k_indices]\n",
        "        \n",
        "        return max(set(k_labels), key=k_labels.count)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "class KNN2:\n",
        "\n",
        "    def __init__(self, dataset, k=5):\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        self.training = dataset.iloc[:,:-1]\n",
        "        self.label = dataset.iloc[:,-1]\n",
        "\n",
        "    def distance(self, vector1, vector2):\n",
        "\n",
        "        dot = numpy.dot\n",
        "        norm = numpy.linalg.norm\n",
        "\n",
        "        cos_similarity = (dot(vector1, vector2) / (norm(vector1) * norm(vector2)))\n",
        "        return (1 - cos_similarity)\n",
        "    \n",
        "    def getDistancesFromPoint(self, vector):\n",
        "\n",
        "        #the vector is essentially a single row in the dataframe\n",
        "        #all attributes except those that have been dropped\n",
        "        #and the label itself are used as part of the cosine distance formula\n",
        "        distances = []\n",
        "\n",
        "        #iterate through rows in dataframe\n",
        "        #for index in self.dataset[0].index:\n",
        "\n",
        "        for index in self.training.index:\n",
        "\n",
        "            #get current point from dataset to get distance from\n",
        "            vector2 = self.training.iloc[index]\n",
        "\n",
        "            #calculate distance\n",
        "            distance = self.distance(vector, vector2)\n",
        "\n",
        "            #append to distance list in dataset\n",
        "            distances.append((distance, self.label[index]))\n",
        "\n",
        "        #sort distances list based on \n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        sorted_labels = [label for distance, label in distances]\n",
        "\n",
        "        #return sorted_labels\n",
        "    \n",
        "    def predict(self, vector):\n",
        "\n",
        "        def findMajority(labels):\n",
        "\n",
        "            #create occurrences array\n",
        "            occurrences = []\n",
        "\n",
        "            #create set of all possible labels\n",
        "            label_types = set(labels)\n",
        "\n",
        "            #iterate through label types\n",
        "            for label in label_types:\n",
        "\n",
        "                #append label and count of label\n",
        "                occurrences.append((label, labels.count(label)))\n",
        "\n",
        "            #find majority label by count using lambda function\n",
        "            majority = max(occurrences, key=lambda count: count[1])\n",
        "\n",
        "            #return majority\n",
        "            return majority\n",
        "\n",
        "        labels = self.getDistancesFromPoint(vector)[:self.k]\n",
        "        return findMajority(labels)[0]\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "distance() missing 2 required positional arguments: 'vector1' and 'vector2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89805/1934800230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtestKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89805/1934800230.py\u001b[0m in \u001b[0;36mtestKNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmajority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajority\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89805/1689191403.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmajority\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDistancesFromPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfindMajority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89805/1689191403.py\u001b[0m in \u001b[0;36mgetDistancesFromPoint\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#return sorted_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: distance() missing 2 required positional arguments: 'vector1' and 'vector2'"
          ]
        }
      ],
      "source": [
        "def testKNN():\n",
        "\n",
        "    data = importData(\"spambase.csv\")\n",
        "    knn = KNN2(data, k=5)\n",
        "\n",
        "    vector1 = data.iloc[:,:-1].iloc[3]\n",
        "    vector2 = data.iloc[:,:-1].iloc[1]\n",
        "\n",
        "    majority = knn.predict(vector2)\n",
        "\n",
        "    print(majority)\n",
        "\n",
        "    \n",
        "\n",
        "testKNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(value):\n",
        "    return 1 / (1 + numpy.exp(-value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addOneColumn(dataset):\n",
        "\n",
        "    #return dataset with new column with all values initialized to one\n",
        "    dataset[\"added_column\"] = 1\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getVectorM(lengthOfDataset):\n",
        "\n",
        "    #vector is represented as an array of numbers\n",
        "    #amount of numbers is determined by number of columns in dataset dataframe\n",
        "    #the vector should be one-dimensional\n",
        "\n",
        "    #first parameter = number of elements in first array\n",
        "    #example code has 1 passed in to signify this is a one-dimensional array\n",
        "\n",
        "    return numpy.random.randn(lengthOfDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitData(dataset):\n",
        "\n",
        "    #drop label column for predicted\n",
        "    dataset = dataset.drop(columns=['spam'])\n",
        "    label = dataset['spam']\n",
        "\n",
        "    return dataset, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance(true, predicted):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linearRegression(dataset):\n",
        "\n",
        "    #add column to dataset\n",
        "    dataset = addOneColumn(dataset)\n",
        "\n",
        "    #create randomized vector m\n",
        "    vector_m = getVectorM(len(dataset.columns))\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        #multiply vector_m by dataset\n",
        "        dataset.dot(vector_m)\n",
        "\n",
        "        #apply sigmoid function\n",
        "        predicted_y = dataset.apply(sigmoid)\n",
        "\n",
        "        #each line will have a predicted y\n",
        "        #it will be 1 for each row\n",
        "        (dataset * (predicted_y - dataset)) * (2 / getDatasetLength(dataset))\n",
        "\n",
        "        print(predicted_y)\n",
        "\n",
        "        currentPerformance = performance()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test():\n",
        "    dataset = importData(\"spambase.csv\")\n",
        "    dataset = removeAttributes(dataset)\n",
        "    dataset = addOneColumn(dataset)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVbjUThpBgf8"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "febo9OPVBhai"
      },
      "outputs": [],
      "source": [
        "def testDataset(training, test):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XoSSUQuVovxL"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  #split the dataset\n",
        "\n",
        "  #use optimize k to find a k value\n",
        "\n",
        "  #build knn graph\n",
        "\n",
        "  #use the validation dataset to test the accuracy of the program\n",
        "\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
