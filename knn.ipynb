{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qdr6G5_tHfZ"
      },
      "source": [
        "questions:\n",
        "1. what is logistic regression used for in this project?\n",
        "2. what is naive bayes used for in this project?\n",
        "3. what metrics and attributes are we using with knn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kphjSEUIw6Y"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dHY5x8MIIyg_"
      },
      "outputs": [],
      "source": [
        "#import pandas for table and csv handling, randomizing dataset\n",
        "import pandas\n",
        "\n",
        "#import numpy for fast math calculations\n",
        "import numpy\n",
        "\n",
        "#import KFold from SKLearn\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#import accuracu from SKLearn\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEvaluation(filename, k=5):\n",
        "\n",
        "    #import data\n",
        "    dataset = importData(filename)\n",
        "\n",
        "    #get split value\n",
        "    split = round(len(dataset.index) * (1/k))\n",
        "    \n",
        "    #shuffle data\n",
        "    dataset = dataset.sample(frac=1)\n",
        "\n",
        "    #split dataset\n",
        "    testing = dataset.loc[:split]\n",
        "    training = dataset.loc[split:]\n",
        "\n",
        "    #use kfold\n",
        "    kfold = KFold(n_splits=k)\n",
        "\n",
        "    #kfold split\n",
        "    for train_index, validation_index in kfold.split(training):\n",
        "\n",
        "        #split into training and validation\n",
        "        sub_valid = training.iloc[validation_index]\n",
        "        sub_train = training.iloc[train_index]\n",
        "\n",
        "        #get spam and not spam probabilities\n",
        "        spam_dict, not_spam_dict = getSpamWordProbabilities(sub_train)\n",
        "        spam_prob = getSpamProbability(sub_train)\n",
        "\n",
        "        #run model evaluation\n",
        "        modelEvaluation(sub_valid, spam_dict, not_spam_dict, spam_prob)\n",
        "\n",
        "        #try knn\n",
        "        knn = KNN(sub_train, k=k)\n",
        "\n",
        "        knn.predict(testing)\n",
        "\n",
        "#crossEvaluation(\"spambase.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQz3hQKuBYIH"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ha0ozsPQJP4d"
      },
      "outputs": [],
      "source": [
        "def importData(filename):\n",
        "\n",
        "  #import file\n",
        "  return pandas.read_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def removeAttributes(dataset):\n",
        "\n",
        "    #drop capital run length average, longest, and total\n",
        "    dataset = dataset.drop(\"capital_run_length_average\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_longest\", axis='columns')\n",
        "    dataset = dataset.drop(\"capital_run_length_total\", axis='columns')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roundToOne(dataset):\n",
        "    return dataset.mask(dataset > 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamOccurrences(dataset):\n",
        "\n",
        "    #positive = spam\n",
        "    #negative = not spam\n",
        "\n",
        "    #get all rows where spam value is set to 1 and 0 respectively\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    #get total occurrences of spam, not spam, and total\n",
        "    spam_length = len(spam.index)\n",
        "    not_spam_length = len(not_spam.index)\n",
        "    total_length = spam_length + not_spam_length\n",
        "\n",
        "    #return tuple with the number of spam and not spam emails in the dataset\n",
        "    return (spam_length, not_spam_length, total_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamProbability(dataset):\n",
        "    spam, not_spam, total = getSpamOccurrences(dataset)\n",
        "    return (spam / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDatasetLength(dataset):\n",
        "    return len(dataset.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordProbabilities(dataset):\n",
        "\n",
        "    #round all non-zero values to 1\n",
        "    dataset = roundToOne(dataset)\n",
        "\n",
        "    #define dictionary for storing word probabilities\n",
        "    word_probability = dict()\n",
        "\n",
        "    #get number of unique features (for laplace smoothing)\n",
        "    unique = len(dataset.columns)\n",
        "\n",
        "    #iterate through columns\n",
        "    for column in dataset.columns:\n",
        "\n",
        "        #sum all values in the current column\n",
        "        #add 1 for laplace smoothing\n",
        "        #divide by the dataset length\n",
        "        #add number of unique values (number of columns)\n",
        "        #to the dataset length for laplace smoothing\n",
        "        word_probability[column] = ((dataset[column].sum() + 1) \n",
        "        / (getDatasetLength(dataset) + unique))\n",
        "\n",
        "    #return word probability dictionary\n",
        "    return word_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSpamWordProbabilities(dataset):\n",
        "\n",
        "    spam = dataset.loc[dataset['spam'] == 1]\n",
        "    not_spam = dataset.loc[dataset['spam'] == 0]\n",
        "\n",
        "    spam_probabilities = getWordProbabilities(spam)\n",
        "    not_spam_probabilities = getWordProbabilities(not_spam)\n",
        "\n",
        "    return (spam_probabilities, not_spam_probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpCcCVBBexF"
      },
      "source": [
        "# KNN\n",
        "Use parallel programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelEvaluation(dataset, spam_probabilities, not_spam_probabilities, spam_probability):\n",
        "\n",
        "    #get spam probability\n",
        "\n",
        "    predicted_list = []\n",
        "\n",
        "    for index, email in dataset.iterrows():\n",
        "\n",
        "        #reset spam predicted\n",
        "        spam_predicted = 1\n",
        "        not_spam_predicted = 1\n",
        "\n",
        "        for column in dataset.iloc[:,:-1]:\n",
        "\n",
        "            if email[column] > 0.:\n",
        "                spam_predicted *= spam_probabilities[column]\n",
        "                not_spam_predicted *= not_spam_probabilities[column]\n",
        "                \n",
        "        spam_predicted = spam_predicted * spam_probability\n",
        "        not_spam_predicted = not_spam_predicted * (1 - spam_probability)\n",
        "\n",
        "        if spam_predicted > not_spam_predicted:\n",
        "            predicted_list.append(1)\n",
        "        else:\n",
        "            predicted_list.append(0)\n",
        "\n",
        "    print(accuracy_score(predicted_list, dataset['spam']))\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNN:\n",
        "\n",
        "    def __init__(self, dataset, k=3):\n",
        "        self.k = k\n",
        "\n",
        "        #x_train = attributes\n",
        "        self.x_train = dataset.iloc[:,:-1]\n",
        "\n",
        "        #y_train = label\n",
        "        self.y_train = dataset.iloc[:,-1]\n",
        "\n",
        "    def _cosine_distance(self, vector1, vector2):\n",
        "\n",
        "        #define subfunction for getting dot product\n",
        "        #a dot product is the sum of the product of all components of two vectors\n",
        "        #print(vector1, vector2)\n",
        "        return (1 - distance.cosine(vector1, vector2))\n",
        "\n",
        "    def predict(self, testing):\n",
        "        testing = testing.iloc[:,:-1]\n",
        "        print(\"starting...\")\n",
        "        distances = cosine_distances(self.x_train, testing)\n",
        "        #get k closest\n",
        "        print(\"ending...\")\n",
        "        print(distances)\n",
        "\n",
        "\n",
        "    def getDistances(self, x):\n",
        "        #compute distance using cosine\n",
        "        distances = [self._cosine_distance(x, x_train) for index, x_train in self.x_train.iterrows()]\n",
        "\n",
        "        #get closest k\n",
        "        k_indices = numpy.argsort(distances)[:self.k]\n",
        "        k_labels = [self.y_train.to_numpy()[index] for index in k_indices]\n",
        "        \n",
        "        return max(set(k_labels), key=k_labels.count)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from multiprocessing import Process, Queue\n",
        "\n",
        "import dill\n",
        "\n",
        "\n",
        "def mp_worker(queue):\n",
        "\n",
        "    while queue.qsize() >0 :\n",
        "        record = queue.get()\n",
        "        print(record)\n",
        "\n",
        "    print(\"worker closed\")\n",
        "\n",
        "class KNN2:\n",
        "\n",
        "    def __init__(self, dataset, k=5):\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        self.training = dataset.iloc[:,:-1]\n",
        "        self.label = dataset.iloc[:,-1]\n",
        "\n",
        "    def distance(self, vector1, vector2):\n",
        "\n",
        "        dot = numpy.dot\n",
        "        norm = numpy.linalg.norm\n",
        "\n",
        "        cos_similarity = (dot(vector1, vector2) / (norm(vector1) * norm(vector2)))\n",
        "        return (1 - cos_similarity)\n",
        "    \n",
        "    def getDistancesFromPoint(self, vector):\n",
        "\n",
        "        #the vector is essentially a single row in the dataframe\n",
        "        #all attributes except those that have been dropped\n",
        "        #and the label itself are used as part of the cosine distance formula\n",
        "        distances = []\n",
        "\n",
        "        #iterate through rows in dataframe\n",
        "        #for index in self.dataset[0].index:\n",
        "\n",
        "        for index in self.training.index:\n",
        "\n",
        "            #get current point from dataset to get distance from\n",
        "            vector2 = self.training.iloc[index]\n",
        "\n",
        "            #calculate distance\n",
        "            distance = self.distance(vector, vector2)\n",
        "\n",
        "            #append to distance list in dataset\n",
        "            distances.append((distance, self.label[index]))\n",
        "\n",
        "        #return distances\n",
        "        return distances\n",
        "    \n",
        "    \n",
        "\n",
        "    def multiDistances(self, vector, num_processes=10):\n",
        "\n",
        "\n",
        "\n",
        "        def splitIntoChunks(dataset, num_processes):\n",
        "\n",
        "            dataset_size = len(dataset.index)\n",
        "            chunk_size = 1#dataset_size // num_processes\n",
        "            chunks = []\n",
        "\n",
        "            #iterate from start to end of dataset by chunk_size\n",
        "            for index in range(0, dataset_size, chunk_size):\n",
        "                chunks.append(dataset.iloc[index:index + chunk_size].numpy())\n",
        "\n",
        "            return chunks\n",
        "\n",
        "        \n",
        "        #chunks = splitIntoChunks(self.training, num_processes)\n",
        "        queue = Queue()\n",
        "        for index in range(0, len(self.training.index)):\n",
        "            queue.put(self.training.iloc[index: index+1])\n",
        "        processes = [Process(target=mp_worker, args=(queue,)) for _ in range(2)]\n",
        "\n",
        "        for process in processes:\n",
        "            process.start()\n",
        "            print('Process started')\n",
        "\n",
        "        for process in processes:\n",
        "            process.join()\n",
        "\n",
        "\n",
        "        #distances = [item for sublist in results for item in sublist]\n",
        "\n",
        "        #pool.close()\n",
        "        #pool.join()\n",
        "\n",
        "        #sort distances list based on \n",
        "\n",
        "        \"\"\"\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        sorted_labels = [label for distance, label in distances]\n",
        "        \"\"\"\n",
        "\n",
        "    def predict(self, vector):\n",
        "\n",
        "        def findMajority(labels):\n",
        "\n",
        "            #create occurrences array\n",
        "            occurrences = []\n",
        "\n",
        "            #create set of all possible labels\n",
        "            label_types = set(labels)\n",
        "\n",
        "            #iterate through label types\n",
        "            for label in label_types:\n",
        "\n",
        "                #append label and count of label\n",
        "                occurrences.append((label, labels.count(label)))\n",
        "\n",
        "            #find majority label by count using lambda function\n",
        "            majority = max(occurrences, key=lambda count: count[1])\n",
        "\n",
        "            #return majority\n",
        "            return majority\n",
        "\n",
        "        #labels = self.getDistancesFromPoint(vector)[:self.k]\n",
        "        self.multiDistances(vector)\n",
        "        #return findMajority(labels)[0]\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process started\n",
            "Process started\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'mp_worker' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/Alex/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'mp_worker' on <module '__main__' (built-in)>\n"
          ]
        }
      ],
      "source": [
        "data = importData(\"spambase.csv\")\n",
        "knn = KNN2(data)\n",
        "\n",
        "def testKNN():\n",
        "\n",
        "    vector1 = data.iloc[:,:-1].iloc[3]\n",
        "    vector2 = data.iloc[:,:-1].iloc[1]\n",
        "\n",
        "    majority = knn.predict(vector2)\n",
        "\n",
        "    print(majority)\n",
        "\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    testKNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linearRegression(dataset):\n",
        "\n",
        "    #add column to dataset\n",
        "    dataset = addOneColumn(dataset)\n",
        "\n",
        "    #create randomized vector m\n",
        "    vector_m = getVectorM(len(dataset.columns))\n",
        "\n",
        "    for i in range(1):\n",
        "\n",
        "        #multiply vector_m by dataset\n",
        "        dataset.dot(vector_m)\n",
        "\n",
        "        #apply sigmoid function\n",
        "        predicted_y = dataset.apply(sigmoid)\n",
        "\n",
        "        #each line will have a predicted y\n",
        "        #it will be 1 for each row\n",
        "        (dataset * (predicted_y - dataset)) * (2 / getDatasetLength(dataset))\n",
        "\n",
        "        print(predicted_y)\n",
        "\n",
        "        currentPerformance = performance()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, dataset, iterations=100):\n",
        "        self.training = dataset.iloc[:,:-1]\n",
        "        self.validation = dataset.iloc[:,:-1]\n",
        "        self.label = dataset.iloc[:,-1]\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def addBias(self):\n",
        "\n",
        "        #add column of entirely 1's to add bias\n",
        "        self.dataset[\"bias\"] = 1\n",
        "\n",
        "        #return modified dataset\n",
        "        return self.dataset\n",
        "\n",
        "    #create sigmoid function\n",
        "    def sigmoid(value):\n",
        "        \n",
        "        #return 1 divided by 1 + e to the power of - value\n",
        "        return 1.0 / ( 1 + numpy.exp( - value ) )\n",
        "    \n",
        "    #create decision function\n",
        "    def decide(value, boundary=0.5):\n",
        "\n",
        "        #if value is greater than boundary, return true\n",
        "        #otherwise, return false\n",
        "        return (value > boundary)\n",
        "    \n",
        "    def performance(self, model):\n",
        "\n",
        "        print(type(model))\n",
        "\n",
        "        return accuracy_score(self.label, self.decide(model))\n",
        "\n",
        "    def getBestModel(self):\n",
        "\n",
        "        #m represents the slope\n",
        "        #to start, we generate a random m\n",
        "        #then, we continue to modify m until we reach a convergence point\n",
        "\n",
        "        def getInitialM(dataset):\n",
        "            return numpy.random.randn(len(dataset.columns))\n",
        "        \n",
        "        def getGradient(dataset, label, predicted_y):\n",
        "            #print(dataset.shape, len(predicted_y), len(label))\n",
        "            return numpy.dot(dataset.T, (predicted_y - label)) * (2 / len(dataset.index))\n",
        "        \n",
        "        vector_m = getInitialM(self.training)\n",
        "        best_performance = 0\n",
        "\n",
        "        #iterate\n",
        "        for iteration in range(self.iterations):\n",
        "\n",
        "            #get predicted y\n",
        "            #print(vector_m.shape, self.training.shape)\n",
        "            predicted_y = numpy.dot(vector_m, self.training.T)\n",
        "\n",
        "            gradient = getGradient(self.training, self.label, predicted_y)\n",
        "            print(gradient)\n",
        "\n",
        "            print(self.performance(gradient))\n",
        "\n",
        "    def predict(self, vector_m):\n",
        "\n",
        "        predicted_y = numpy.dot(vector_m, self.training.T)\n",
        "        return self.sigmoid(predicted_y)\n",
        "\n",
        "        #model is a vector of 57 columns, \n",
        "        #the result of the dot product of the dataset and the predicted_y\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class LogisticRegression2:\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, iterations=2000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterartions = iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.threshold\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(value):\n",
        "        return 1 / (1 + np.exp( - value))\n",
        "\n",
        "    def fit(self, data, label):\n",
        "\n",
        "        sigmoid = LogisticRegression2.sigmoid\n",
        "\n",
        "        samples, features = data.shape\n",
        "        self.weights = [0] * features\n",
        "        self.bias = 0\n",
        "\n",
        "        for iteration in range(self.iterartions):\n",
        "            linear_pred = np.dot(data, self.weights) + self.bias\n",
        "            predictions = sigmoid(linear_pred)\n",
        "\n",
        "            gradient_weight = (1/samples) * np.dot(data.T, (predictions - label))\n",
        "            gradient_bias = (1/samples) * np.sum(predictions - label)\n",
        "\n",
        "            self.weights = self.weights - self.learning_rate * gradient_weight\n",
        "            self.bias = self.bias - self.learning_rate * gradient_bias\n",
        "\n",
        "\n",
        "    def predict(self, data, threshold=0.5):\n",
        "\n",
        "        sigmoid = LogisticRegression2.sigmoid\n",
        "\n",
        "        linear_pred = np.dot(data, self.weights) + self.bias\n",
        "        y_predictions = sigmoid(linear_pred)\n",
        "\n",
        "        predictions = [0 if y <= threshold else 1 for y in y_predictions]\n",
        "\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitData(dataset):\n",
        "\n",
        "    x = dataset.iloc[:,:-1]\n",
        "    y = dataset.iloc[:,-1]\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "    data = importData(\"spambase.csv\")\n",
        "    test_data, test_label = splitData(data)\n",
        "\n",
        "    def accuracy(y_pred, y_label):\n",
        "        return np.sum(y_pred==y_label) / len(y_label)\n",
        "\n",
        "\n",
        "    lr = LogisticRegression2()\n",
        "    lr.fit(data, label)\n",
        "    predictions = lr.predict(test_data, test_label)\n",
        "\n",
        "    acc = accuracy(predictions, y_label) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.83634502e+02 -1.73199498e+02 -4.10022947e+02 -1.44648132e+02\n",
            " -3.65783838e+02 -1.65330845e+02 -1.27529283e+02 -1.59338817e+02\n",
            " -2.71578827e+02 -4.14471036e+02 -1.24288533e+02 -6.53912392e+02\n",
            " -1.80069790e+02 -1.98790413e+02 -1.59380661e+02 -2.92140424e+02\n",
            " -2.32391282e+02 -2.74343505e+02 -1.83959848e+03 -1.97992055e+02\n",
            " -1.07687357e+03 -3.82053950e+02 -2.56758801e+02 -1.91357464e+02\n",
            " -4.40426333e+02 -1.70526513e+02 -8.23922397e+01 -5.56059263e+01\n",
            " -3.09785673e+01 -4.72986865e+01 -3.01083222e+01 -1.75258727e+01\n",
            " -1.13578677e+02 -2.02679448e+01 -6.15156609e+01 -6.58572997e+01\n",
            " -1.46996441e+02 -7.34157925e+00 -3.68651457e+01 -5.14283358e+01\n",
            " -2.54818516e+01 -4.30029079e+01 -3.27430106e+01 -2.72485751e+01\n",
            " -1.10425338e+02 -9.82880736e+01 -6.78272259e+00 -2.80072174e+01\n",
            " -7.63819720e+01 -2.54599659e+02 -2.01749278e+01 -3.88280340e+02\n",
            " -2.09338280e+02 -9.82011261e+01 -2.16292465e+04 -3.22543097e+05\n",
            " -1.75938952e+06]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'float' and 'LogisticRegression'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89878/3161228387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89878/3161228387.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spambase.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89878/1400305170.py\u001b[0m in \u001b[0;36mgetBestModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89878/1400305170.py\u001b[0m in \u001b[0;36mperformance\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetBestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/t5/mrs43bys15d9807_bsdf2vpw0000gn/T/ipykernel_89878/1400305170.py\u001b[0m in \u001b[0;36mdecide\u001b[0;34m(value, boundary)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#if value is greater than boundary, return true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#otherwise, return false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'LogisticRegression'"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    dataset = importData(\"spambase.csv\")\n",
        "    lr = LogisticRegression(dataset, iterations=1)\n",
        "    lr.getBestModel()\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVbjUThpBgf8"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "febo9OPVBhai"
      },
      "outputs": [],
      "source": [
        "def testDataset(training, test):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XoSSUQuVovxL"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  #split the dataset\n",
        "\n",
        "  #use optimize k to find a k value\n",
        "\n",
        "  #build knn graph\n",
        "\n",
        "  #use the validation dataset to test the accuracy of the program\n",
        "\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
